{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlp_week1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOmTaMupByK37vRVj+n2v2A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venomouscyanide/dl_sain/blob/master/week1/mlp_week1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQhY-9Juoxu9",
        "outputId": "072d7a15-f0f3-4bdf-ce6d-130fa99c230b"
      },
      "source": [
        "# gdown included in the env is outdated. Install the latest release\n",
        "!pip install gdown==3.13.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown==3.13.0 in /usr/local/lib/python3.7/dist-packages (3.13.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown==3.13.0) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.12.0 in /usr/local/lib/python3.7/dist-packages (from gdown==3.13.0) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown==3.13.0) (3.0.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown==3.13.0) (4.41.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.12.0->gdown==3.13.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.12.0->gdown==3.13.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.12.0->gdown==3.13.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.12.0->gdown==3.13.0) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.12.0->gdown==3.13.0) (1.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnnciY3y3cmG"
      },
      "source": [
        "### Load all essential libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8poeFsf3-eA"
      },
      "source": [
        "import gzip\n",
        "import shutil\n",
        "from typing import Tuple, List\n",
        "# third party\n",
        "import gdown # helps download gdrive files\n",
        "import numpy as np\n",
        "import struct\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xFbK7rU4MKD"
      },
      "source": [
        "### Initialize class for loading MNIST data. MNIST data being downloaded is a mirror of the [original MNIST dataset](http://yann.lecun.com/exdb/mnist/)\n",
        "\n",
        "Responsibilities:\n",
        "\n",
        "1.   Download the original compressed datasets\n",
        "2.   Uncompress and write the uncompressed files\n",
        "3.   Read the uncompressed idx files and convert them to an ndarray\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_3gluuPJpJ_"
      },
      "source": [
        "class MNISTDataLoader:\n",
        "    # TODO: Do all this in memory.\n",
        "    TRAINING_DATA_URL: str = 'https://drive.google.com/uc?id=1pmI9wAdNtJkOvkJpdTqM9bmIAwPkGyMU'\n",
        "    TRAINING_DATA_LABELS_URL: str = 'https://drive.google.com/uc?id=1R8BZL67U1N0GUGnf6AQIBZNVDCWO9QLS'\n",
        "    TESTING_DATA_URL: str = 'https://drive.google.com/uc?id=10FdcUHw3BcQAU6keKaUwtDwJm4sC00Hu'\n",
        "    TESTING_DATA_LABELS_URL: str = 'https://drive.google.com/uc?id=1GvsacEnI1eQ1vYZM-oYdERvaE2SPh0Lj'\n",
        "\n",
        "    def load_data_wrapper(self):\n",
        "        testing_data_tuple = self.load_data_as_ndarray(self.TESTING_DATA_URL, self.TESTING_DATA_LABELS_URL, False)\n",
        "        training_data_tuple = self.load_data_as_ndarray(self.TRAINING_DATA_URL, self.TRAINING_DATA_LABELS_URL, True)\n",
        "        return training_data_tuple, testing_data_tuple\n",
        "\n",
        "    def load_data_as_ndarray(self, data_file_url: str, data_labels_file_url: str, train: bool) -> List[\n",
        "        Tuple[np.ndarray, int]]:\n",
        "        uncompressed_dataset = self._download_and_uncompressed_file(data_file_url)\n",
        "        uncompressed_labels = self._download_and_uncompressed_file(data_labels_file_url)\n",
        "        pixel_data = self._get_pixel_data(uncompressed_dataset)\n",
        "        label_data = self._get_labels(uncompressed_labels)\n",
        "        zipped_data = [\n",
        "            (x.reshape(784, 1), self._one_hot_enc(y) if train else y[0]) for x, y in zip(pixel_data, label_data)\n",
        "        ]\n",
        "        return zipped_data\n",
        "\n",
        "    def _one_hot_enc(self, y: np.ndarray):\n",
        "        one_hot_vector = np.zeros((10, 1))\n",
        "        one_hot_vector[y[0]][0] = 1\n",
        "        return one_hot_vector\n",
        "\n",
        "    def _download_and_uncompressed_file(self, url: str) -> str:\n",
        "        downloaded_gzip = gdown.download(url, quiet=True)\n",
        "        decompressed_data_file = self._write_decompressed_data(downloaded_gzip)\n",
        "        return decompressed_data_file\n",
        "\n",
        "    def _write_decompressed_data(self, downloaded_gzip: str) -> str:\n",
        "        with gzip.open(downloaded_gzip, 'rb') as compressed:\n",
        "            uncompressed_dataset = downloaded_gzip.replace('.gz', '')\n",
        "            with open(uncompressed_dataset, 'wb') as decompressed:\n",
        "                shutil.copyfileobj(compressed, decompressed)\n",
        "        return uncompressed_dataset\n",
        "\n",
        "    def _get_pixel_data(self, data_file: str) -> np.ndarray:\n",
        "        with open(data_file, \"rb\") as dataset:\n",
        "            _, num_data = struct.unpack(\">II\", dataset.read(8))\n",
        "            num_rows, num_colums = struct.unpack(\">II\", dataset.read(8))\n",
        "            pixel_data = np.fromfile(dataset, dtype=np.uint8) / 255\n",
        "            pixel_data = pixel_data.reshape((num_data, num_rows * num_colums))\n",
        "        return pixel_data\n",
        "\n",
        "    def _get_labels(self, data_labels_file: str) -> np.ndarray:\n",
        "        with open(data_labels_file, \"rb\") as labels:\n",
        "            _, num_data = struct.unpack(\">II\", labels.read(8))\n",
        "            label_data = np.fromfile(labels, dtype=np.uint8)\n",
        "            label_data = label_data.reshape((num_data, -1))\n",
        "        return label_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JmnFUMF4mr4"
      },
      "source": [
        "### Utility methods needed for building the neural net\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z2j3tJ7J1wa"
      },
      "source": [
        "class NetworkUtils:\n",
        "    @staticmethod\n",
        "    def sigmoid(z: np.ndarray) -> np.ndarray:\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid_prime(z: np.ndarray) -> np.ndarray:\n",
        "        return NetworkUtils.sigmoid(z) * (1 - NetworkUtils.sigmoid(z))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXBjPMbn4tTt"
      },
      "source": [
        "### The neural network itself\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkK8wFhhJ4hT"
      },
      "source": [
        "class Network:\n",
        "    def __init__(self, training_data: List[Tuple[np.ndarray, np.ndarray]],\n",
        "                 testing_data: List[Tuple[np.ndarray, int]],\n",
        "                 size: List[int], learning_rate: float, epochs: int,\n",
        "                 mini_batch_size: int):\n",
        "        self.training_data = training_data\n",
        "        self.testing_data = testing_data\n",
        "        self.size = size\n",
        "        self.num_layers = len(size)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.biases = []\n",
        "        self.weights = []\n",
        "        self._init_biases()\n",
        "        self._init_weights()\n",
        "        self.epochs = epochs\n",
        "        self.mini_batch_size = mini_batch_size\n",
        "\n",
        "    def _init_biases(self):\n",
        "        for i in range(1, self.num_layers):\n",
        "            self.biases.append(np.random.randn(self.size[i], 1))\n",
        "\n",
        "    def _init_weights(self):\n",
        "        bias_matrix_sizes = [(self.size[x + 1], self.size[x]) for x in range(self.num_layers - 1)]\n",
        "        for x, y in bias_matrix_sizes:\n",
        "            self.weights.append(np.random.randn(x, y))\n",
        "\n",
        "    def train(self):\n",
        "        for epoch in range(1, self.epochs + 1):\n",
        "            np.random.shuffle(self.training_data)\n",
        "          \n",
        "            mini_batches = self._create_mini_batches()\n",
        "            for mini_batch in mini_batches:\n",
        "                self._update_b_w(mini_batch)\n",
        "            \n",
        "            if epoch % 10 == 0:\n",
        "              print(f\"Finish training for epoch: {epoch} of {self.epochs}\")\n",
        "              self._calc_accuracy()\n",
        "\n",
        "    def _create_mini_batches(self) -> List[List[Tuple[np.ndarray, np.ndarray]]]:\n",
        "        mini_batches = [\n",
        "            self.training_data[multiple:multiple + self.mini_batch_size] for multiple in\n",
        "            range(0, len(self.training_data), self.mini_batch_size)\n",
        "        ]\n",
        "        return mini_batches\n",
        "\n",
        "    def _update_b_w(self, mini_batch: List[Tuple[np.ndarray, np.ndarray]]):\n",
        "        nabla_bias = self._get_nabla_bias_zeroes()\n",
        "        nabla_wt = self._get_nabla_wt_zeroes()\n",
        "\n",
        "        for x, y in mini_batch:\n",
        "            del_bias, del_wt = self._run_back_propagation(x, y)\n",
        "\n",
        "            nabla_bias = [curr_b + del_b for curr_b, del_b in zip(nabla_bias, del_bias)]\n",
        "            nabla_wt = [curr_wt + del_w for curr_wt, del_w in zip(nabla_wt, del_wt)]\n",
        "\n",
        "        self.biases = [\n",
        "            b - ((self.learning_rate / self.mini_batch_size) * nb) for b, nb in zip(self.biases, nabla_bias)\n",
        "        ]\n",
        "        self.weights = [\n",
        "            w - ((self.learning_rate / self.mini_batch_size) * nw) for w, nw in zip(self.weights, nabla_wt)\n",
        "        ]\n",
        "\n",
        "    def _get_nabla_bias_zeroes(self) -> List[np.ndarray]:\n",
        "        return [np.zeros(np.shape(bias)) for bias in self.biases]\n",
        "\n",
        "    def _get_nabla_wt_zeroes(self) -> List[np.ndarray]:\n",
        "        return [np.zeros(np.shape(wt)) for wt in self.weights]\n",
        "\n",
        "    def _run_back_propagation(self, x: np.ndarray, y: np.ndarray) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
        "        nabla_bias = self._get_nabla_bias_zeroes()\n",
        "        nabla_wt = self._get_nabla_wt_zeroes()\n",
        "\n",
        "        activations = []\n",
        "        z_list = []\n",
        "\n",
        "        a = x\n",
        "        activations.append(a)\n",
        "\n",
        "        for i in range(self.num_layers - 1):\n",
        "            z = np.dot(self.weights[i], a) + self.biases[i]\n",
        "            z_list.append(z)\n",
        "\n",
        "            a = NetworkUtils.sigmoid(z)\n",
        "            activations.append(a)\n",
        "\n",
        "        error_l = np.multiply(self._nabla_a(activations[-1], y), NetworkUtils.sigmoid_prime(z_list[-1]))\n",
        "        nabla_bias[-1] = error_l\n",
        "        nabla_wt[-1] = np.dot(error_l, np.transpose(activations[-2]))\n",
        "\n",
        "        for layer in range(self.num_layers - 2, 0, -1):\n",
        "            error_l = np.multiply(\n",
        "                np.dot(np.transpose(self.weights[layer]), error_l), NetworkUtils.sigmoid_prime(z_list[layer - 1])\n",
        "            )\n",
        "\n",
        "            nabla_bias[layer - 1] = error_l\n",
        "            nabla_wt[layer - 1] = np.dot(error_l, activations[layer - 1].transpose())\n",
        "\n",
        "        return nabla_bias, nabla_wt\n",
        "\n",
        "    def _nabla_a(self, a_l: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
        "        return a_l - y\n",
        "\n",
        "    def _calc_accuracy(self):\n",
        "        correct_results = 0\n",
        "        total_results = len(self.testing_data)\n",
        "        for x, y in self.testing_data:\n",
        "            logit = self.feedforward(x)\n",
        "            if np.argmax(logit) == y:\n",
        "                correct_results += 1\n",
        "        print(f\"Total accuracy on testing data: {round((correct_results / total_results) * 100, 2)}%\")\n",
        "\n",
        "    def feedforward(self, x: np.ndarray) -> np.ndarray:\n",
        "        a = x\n",
        "        for layer in range(self.num_layers - 1):\n",
        "            a = NetworkUtils.sigmoid(np.dot(self.weights[layer], a) + self.biases[layer])\n",
        "        return a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzbQI22N42YZ"
      },
      "source": [
        "### Initialize all the hyperparameters "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpfqF93rJ6-i"
      },
      "source": [
        "class Hyperparameters:\n",
        "    SIZE: List[int] = [28 * 28, 30, 10]\n",
        "    LEARNING_RATE: float = 5\n",
        "    EPOCHS: int = 50\n",
        "    MINI_BATCH_SIZE: int = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiwH5N6345DB"
      },
      "source": [
        "### Driver method for training and evaluating the performance on each epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1MeNMvGJ_sR"
      },
      "source": [
        "def train_and_eval():\n",
        "    print(\"Loading data\")\n",
        "    training, testing = MNISTDataLoader().load_data_wrapper()\n",
        "    params = Hyperparameters\n",
        "    print(\"Training NN and testing accuracy\")\n",
        "    mlp = Network(training, testing, params.SIZE, params.LEARNING_RATE, params.EPOCHS, params.MINI_BATCH_SIZE)\n",
        "    mlp.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1duv-mr4-_x"
      },
      "source": [
        "### Call the driver method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5u2Dv02SLPww",
        "outputId": "4f11b7e1-2cb7-4a3f-98ca-111c39e32b3a"
      },
      "source": [
        "train_and_eval()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data\n",
            "Training NN and testing accuracy\n",
            "Finish training for epoch: 10 of 50\n",
            "Total accuracy on testing data: 94.38%\n",
            "Finish training for epoch: 20 of 50\n",
            "Total accuracy on testing data: 94.85%\n",
            "Finish training for epoch: 30 of 50\n",
            "Total accuracy on testing data: 95.23%\n",
            "Finish training for epoch: 40 of 50\n",
            "Total accuracy on testing data: 95.38%\n",
            "Finish training for epoch: 50 of 50\n",
            "Total accuracy on testing data: 95.24%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}