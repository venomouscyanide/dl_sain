{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlp_week1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNheLt0/QguXpK1n6CFProd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venomouscyanide/dl_sain/blob/master/week1/mlp_week1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_3gluuPJpJ_"
      },
      "source": [
        "import pickle\n",
        "import gzip\n",
        "\n",
        "# Third-party libraries\n",
        "import numpy as np\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Return the MNIST data as a tuple containing the training data,\n",
        "    the validation data, and the test data.\n",
        "    The ``training_data`` is returned as a tuple with two entries.\n",
        "    The first entry contains the actual training images.  This is a\n",
        "    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n",
        "    numpy ndarray with 784 values, representing the 28 * 28 = 784\n",
        "    pixels in a single MNIST image.\n",
        "    The second entry in the ``training_data`` tuple is a numpy ndarray\n",
        "    containing 50,000 entries.  Those entries are just the digit\n",
        "    values (0...9) for the corresponding images contained in the first\n",
        "    entry of the tuple.\n",
        "    The ``validation_data`` and ``test_data`` are similar, except\n",
        "    each contains only 10,000 images.\n",
        "    This is a nice data format, but for use in neural networks it's\n",
        "    helpful to modify the format of the ``training_data`` a little.\n",
        "    That's done in the wrapper function ``load_data_wrapper()``, see\n",
        "    below.\n",
        "    \"\"\"\n",
        "    f = gzip.open('/content/drive/MyDrive/Colab Notebooks/mnist.pkl.gz', 'rb')\n",
        "    training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
        "    f.close()\n",
        "    return (training_data, validation_data, test_data)\n",
        "\n",
        "def load_data_wrapper():\n",
        "    \"\"\"Return a tuple containing ``(training_data, validation_data,\n",
        "    test_data)``. Based on ``load_data``, but the format is more\n",
        "    convenient for use in our implementation of neural networks.\n",
        "    In particular, ``training_data`` is a list containing 50,000\n",
        "    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray\n",
        "    containing the input image.  ``y`` is a 10-dimensional\n",
        "    numpy.ndarray representing the unit vector corresponding to the\n",
        "    correct digit for ``x``.\n",
        "    ``validation_data`` and ``test_data`` are lists containing 10,000\n",
        "    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional\n",
        "    numpy.ndarry containing the input image, and ``y`` is the\n",
        "    corresponding classification, i.e., the digit values (integers)\n",
        "    corresponding to ``x``.\n",
        "    Obviously, this means we're using slightly different formats for\n",
        "    the training data and the validation / test data.  These formats\n",
        "    turn out to be the most convenient for use in our neural network\n",
        "    code.\"\"\"\n",
        "    tr_d, va_d, te_d = load_data()\n",
        "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
        "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
        "    training_data = zip(training_inputs, training_results)\n",
        "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
        "    validation_data = zip(validation_inputs, va_d[1])\n",
        "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
        "    test_data = zip(test_inputs, te_d[1])\n",
        "    return (training_data, validation_data, test_data)\n",
        "\n",
        "def vectorized_result(j):\n",
        "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
        "    position and zeroes elsewhere.  This is used to convert a digit\n",
        "    (0...9) into a corresponding desired output from the neural\n",
        "    network.\"\"\"\n",
        "    e = np.zeros((10, 1))\n",
        "    e[j] = 1.0\n",
        "    return e\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI6Ghu4TJsvR"
      },
      "source": [
        "from typing import List, Tuple\n",
        "import numpy as np"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z2j3tJ7J1wa"
      },
      "source": [
        "class NetworkUtils:\n",
        "    @staticmethod\n",
        "    def sigmoid(z: np.ndarray) -> np.ndarray:\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid_prime(z: np.ndarray) -> np.ndarray:\n",
        "        return NetworkUtils.sigmoid(z) * (1 - NetworkUtils.sigmoid(z))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkK8wFhhJ4hT"
      },
      "source": [
        "class Network:\n",
        "    def __init__(self, training_data: List[Tuple[np.ndarray, np.ndarray]],\n",
        "                 testing_data: List[Tuple[np.ndarray, int]],\n",
        "                 size: List[int], learning_rate: float, epochs: int,\n",
        "                 mini_batch_size: int):\n",
        "        self.training_data = training_data\n",
        "        self.testing_data = testing_data\n",
        "        self.size = size\n",
        "        self.num_layers = len(size)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.biases = []\n",
        "        self.weights = []\n",
        "        self._init_biases()\n",
        "        self._init_weights()\n",
        "        self.epochs = epochs\n",
        "        self.mini_batch_size = mini_batch_size\n",
        "\n",
        "    def _init_biases(self):\n",
        "        for i in range(1, self.num_layers):\n",
        "            self.biases.append(np.random.randn(self.size[i], 1))\n",
        "\n",
        "    def _init_weights(self):\n",
        "        bias_matrix_sizes = [(self.size[x + 1], self.size[x]) for x in range(self.num_layers - 1)]\n",
        "        for x, y in bias_matrix_sizes:\n",
        "            self.weights.append(np.random.randn(x, y))\n",
        "\n",
        "    def train(self):\n",
        "        for epoch in range(self.epochs):\n",
        "            np.random.shuffle(self.training_data)\n",
        "            print(f\"Start training for epoch: {epoch + 1} of {self.epochs}\")\n",
        "\n",
        "            num_mini_batches = len(self.training_data) // self.mini_batch_size\n",
        "            mini_batches = self._create_mini_batches()\n",
        "\n",
        "            for batch, mini_batch in enumerate(mini_batches, start=1):\n",
        "                if batch % 1000 == 0:\n",
        "                    print(f\"calculating SGD for: Batch {batch}/{num_mini_batches} of epoch: {epoch}\")\n",
        "                self._update_b_w(mini_batch)\n",
        "            self._calc_accuracy()\n",
        "\n",
        "    def _create_mini_batches(self) -> List[List[Tuple[np.ndarray, np.ndarray]]]:\n",
        "        mini_batches = [\n",
        "            self.training_data[multiple:multiple + self.mini_batch_size] for multiple in\n",
        "            range(0, len(self.training_data), self.mini_batch_size)\n",
        "        ]\n",
        "        return mini_batches\n",
        "\n",
        "    def _update_b_w(self, mini_batch: List[Tuple[np.ndarray, np.ndarray]]):\n",
        "        nabla_bias = self._get_nabla_bias_zeroes()\n",
        "        nabla_wt = self._get_nabla_wt_zeroes()\n",
        "\n",
        "        for x, y in mini_batch:\n",
        "            del_bias, del_wt = self._run_back_propagation(x, y)\n",
        "\n",
        "            nabla_bias = [curr_b + del_b for curr_b, del_b in zip(nabla_bias, del_bias)]\n",
        "            nabla_wt = [curr_wt + del_w for curr_wt, del_w in zip(nabla_wt, del_wt)]\n",
        "\n",
        "        self.biases = [\n",
        "            b - ((self.learning_rate / self.mini_batch_size) * nb) for b, nb in zip(self.biases, nabla_bias)\n",
        "        ]\n",
        "        self.weights = [\n",
        "            w - ((self.learning_rate / self.mini_batch_size) * nw) for w, nw in zip(self.weights, nabla_wt)\n",
        "        ]\n",
        "\n",
        "    def _get_nabla_bias_zeroes(self) -> List[np.ndarray]:\n",
        "        return [np.zeros(np.shape(bias)) for bias in self.biases]\n",
        "\n",
        "    def _get_nabla_wt_zeroes(self) -> List[np.ndarray]:\n",
        "        return [np.zeros(np.shape(wt)) for wt in self.weights]\n",
        "\n",
        "    def _run_back_propagation(self, x: np.ndarray, y: np.ndarray) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
        "        nabla_bias = self._get_nabla_bias_zeroes()\n",
        "        nabla_wt = self._get_nabla_wt_zeroes()\n",
        "\n",
        "        activations = []\n",
        "        z_list = []\n",
        "\n",
        "        a = x\n",
        "        activations.append(a)\n",
        "\n",
        "        for i in range(self.num_layers - 1):\n",
        "            z = np.dot(self.weights[i], a) + self.biases[i]\n",
        "            z_list.append(z)\n",
        "\n",
        "            a = NetworkUtils.sigmoid(z)\n",
        "            activations.append(a)\n",
        "\n",
        "        error_l = np.multiply(self._nabla_a(activations[-1], y), NetworkUtils.sigmoid_prime(z_list[-1]))\n",
        "        nabla_bias[-1] = error_l\n",
        "        nabla_wt[-1] = np.dot(error_l, np.transpose(activations[-2]))\n",
        "\n",
        "        for layer in range(self.num_layers - 2, 0, -1):\n",
        "            error_l = np.multiply(\n",
        "                np.dot(np.transpose(self.weights[layer]), error_l), NetworkUtils.sigmoid_prime(z_list[layer - 1])\n",
        "            )\n",
        "\n",
        "            nabla_bias[layer - 1] = error_l\n",
        "            nabla_wt[layer - 1] = np.dot(error_l, activations[layer - 1].transpose())\n",
        "\n",
        "        return nabla_bias, nabla_wt\n",
        "\n",
        "    def _nabla_a(self, a_l: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
        "        return a_l - y\n",
        "\n",
        "    def _calc_accuracy(self):\n",
        "        correct_results = 0\n",
        "        total_results = len(self.testing_data)\n",
        "        for x, y in self.testing_data:\n",
        "            logit = self.feedforward(x)\n",
        "            if np.argmax(logit) == y:\n",
        "                correct_results += 1\n",
        "        print(f\"Total accuracy on testing data: {round((correct_results / total_results) * 100, 2)}\")\n",
        "\n",
        "    def feedforward(self, x: np.ndarray) -> np.ndarray:\n",
        "        a = x\n",
        "        for layer in range(self.num_layers - 1):\n",
        "            a = NetworkUtils.sigmoid(np.dot(self.weights[layer], a) + self.biases[layer])\n",
        "        return a"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpfqF93rJ6-i"
      },
      "source": [
        "class Hyperparameters:\n",
        "    SIZE: List[int] = [28 * 28, 30, 10]\n",
        "    LEARNING_RATE: float = 10\n",
        "    EPOCHS: int = 15\n",
        "    MINI_BATCH_SIZE: int = 10"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1MeNMvGJ_sR"
      },
      "source": [
        "def train_and_eval():\n",
        "    training, _, testing = load_data_wrapper()\n",
        "    params = Hyperparameters\n",
        "    mlp = Network(list(training), list(testing), params.SIZE, params.LEARNING_RATE, params.EPOCHS,\n",
        "                  params.MINI_BATCH_SIZE)\n",
        "    mlp.train()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5u2Dv02SLPww",
        "outputId": "35e36e49-c4b9-439d-a67a-924db6dfaf83"
      },
      "source": [
        "train_and_eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training for epoch: 1 of 15\n",
            "calculating SGD for: Batch 1000/5000 of epoch: 0\n",
            "calculating SGD for: Batch 2000/5000 of epoch: 0\n",
            "calculating SGD for: Batch 3000/5000 of epoch: 0\n",
            "calculating SGD for: Batch 4000/5000 of epoch: 0\n",
            "calculating SGD for: Batch 5000/5000 of epoch: 0\n",
            "Total accuracy on testing data: 84.41\n",
            "Start training for epoch: 2 of 15\n",
            "calculating SGD for: Batch 1000/5000 of epoch: 1\n",
            "calculating SGD for: Batch 2000/5000 of epoch: 1\n",
            "calculating SGD for: Batch 3000/5000 of epoch: 1\n",
            "calculating SGD for: Batch 4000/5000 of epoch: 1\n",
            "calculating SGD for: Batch 5000/5000 of epoch: 1\n",
            "Total accuracy on testing data: 89.57\n",
            "Start training for epoch: 3 of 15\n",
            "calculating SGD for: Batch 1000/5000 of epoch: 2\n",
            "calculating SGD for: Batch 2000/5000 of epoch: 2\n",
            "calculating SGD for: Batch 3000/5000 of epoch: 2\n",
            "calculating SGD for: Batch 4000/5000 of epoch: 2\n",
            "calculating SGD for: Batch 5000/5000 of epoch: 2\n",
            "Total accuracy on testing data: 93.02\n",
            "Start training for epoch: 4 of 15\n",
            "calculating SGD for: Batch 1000/5000 of epoch: 3\n",
            "calculating SGD for: Batch 2000/5000 of epoch: 3\n",
            "calculating SGD for: Batch 3000/5000 of epoch: 3\n",
            "calculating SGD for: Batch 4000/5000 of epoch: 3\n",
            "calculating SGD for: Batch 5000/5000 of epoch: 3\n",
            "Total accuracy on testing data: 92.96\n",
            "Start training for epoch: 5 of 15\n",
            "calculating SGD for: Batch 1000/5000 of epoch: 4\n",
            "calculating SGD for: Batch 2000/5000 of epoch: 4\n",
            "calculating SGD for: Batch 3000/5000 of epoch: 4\n",
            "calculating SGD for: Batch 4000/5000 of epoch: 4\n",
            "calculating SGD for: Batch 5000/5000 of epoch: 4\n",
            "Total accuracy on testing data: 93.8\n",
            "Start training for epoch: 6 of 15\n",
            "calculating SGD for: Batch 1000/5000 of epoch: 5\n",
            "calculating SGD for: Batch 2000/5000 of epoch: 5\n",
            "calculating SGD for: Batch 3000/5000 of epoch: 5\n",
            "calculating SGD for: Batch 4000/5000 of epoch: 5\n",
            "calculating SGD for: Batch 5000/5000 of epoch: 5\n",
            "Total accuracy on testing data: 93.75\n",
            "Start training for epoch: 7 of 15\n",
            "calculating SGD for: Batch 1000/5000 of epoch: 6\n",
            "calculating SGD for: Batch 2000/5000 of epoch: 6\n",
            "calculating SGD for: Batch 3000/5000 of epoch: 6\n",
            "calculating SGD for: Batch 4000/5000 of epoch: 6\n",
            "calculating SGD for: Batch 5000/5000 of epoch: 6\n",
            "Total accuracy on testing data: 93.83\n",
            "Start training for epoch: 8 of 15\n",
            "calculating SGD for: Batch 1000/5000 of epoch: 7\n",
            "calculating SGD for: Batch 2000/5000 of epoch: 7\n",
            "calculating SGD for: Batch 3000/5000 of epoch: 7\n",
            "calculating SGD for: Batch 4000/5000 of epoch: 7\n",
            "calculating SGD for: Batch 5000/5000 of epoch: 7\n",
            "Total accuracy on testing data: 94.29\n",
            "Start training for epoch: 9 of 15\n",
            "calculating SGD for: Batch 1000/5000 of epoch: 8\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}