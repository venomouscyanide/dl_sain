{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "colab": {
      "name": "week6_3a_3b.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venomouscyanide/dl_sain/blob/master/week6/week6_3a_3b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDDASr4lex05"
      },
      "source": [
        "NB: All scripts were run locally due to compute constraints faced on Colab. This notebook is maintaned only to showcase the outputs/main parts of the code. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkI7muYwdmyp"
      },
      "source": [
        "### Get all imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "s-L4hqW8dmyr"
      },
      "source": [
        "import math\n",
        "import os\n",
        "import string\n",
        "from collections import Counter, defaultdict\n",
        "from math import ceil\n",
        "from typing import List, Set, Tuple, Type, Dict\n",
        "from abc import abstractmethod\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import gdown\n",
        "import unidecode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0tn0RnEdmys"
      },
      "source": [
        "### Init the class which will give the password datasets. Currently supports Mate1, 000webhost and ClixSense."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "PFmW4sVSdmys"
      },
      "source": [
        "class PasswordDataset:\n",
        "    @abstractmethod\n",
        "    def get_download_url(self) -> str:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_dataset_local_path(self) -> str:\n",
        "        pass\n",
        "\n",
        "\n",
        "class ClixSense(PasswordDataset):\n",
        "    def get_download_url(self) -> str:\n",
        "        return 'https://drive.google.com/uc?id=1S0-1gdzoP-HecS3L5_zStZhvTt9A4q97'\n",
        "\n",
        "    def get_dataset_local_path(self) -> str:\n",
        "        return 'pwd_dataset_manager/datasets/ClixSense.txt'\n",
        "\n",
        "\n",
        "class WebHost(PasswordDataset):\n",
        "    def get_download_url(self) -> str:\n",
        "        return 'https://drive.google.com/uc?id=11tsLveuHo3xaVL2DRh3FfuUPG8LEtzYd'\n",
        "\n",
        "    def get_dataset_local_path(self) -> str:\n",
        "        return 'pwd_dataset_manager/datasets/000webhost.txt'\n",
        "\n",
        "\n",
        "class Mate1(PasswordDataset):\n",
        "    def get_download_url(self) -> str:\n",
        "        return 'https://drive.google.com/uc?id=10LtJiV9J-Vuy1I8iSxacH4fsPqZamIeB'\n",
        "\n",
        "    def get_dataset_local_path(self) -> str:\n",
        "        return 'pwd_dataset_manager/datasets/Mate1.txt'\n",
        "\n",
        "\n",
        "class DatasetFactory:\n",
        "    def get(self, dataset_name: str) -> Type[PasswordDataset]:\n",
        "        if dataset_name == \"ClixSense\":\n",
        "            return ClixSense\n",
        "        elif dataset_name == \"000webhost\":\n",
        "            return WebHost\n",
        "        elif dataset_name == \"Mate1\":\n",
        "            return Mate1\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Dataset: {dataset_name} not supported\")\n",
        "\n",
        "\n",
        "def get_dataset(dataset_klass: Type[PasswordDataset]) -> List[str]:\n",
        "    local_dataset = dataset_klass().get_dataset_local_path()\n",
        "    if not os.path.exists(local_dataset):\n",
        "        gdown.download(dataset_klass().get_download_url(), quiet=True, output=local_dataset)\n",
        "\n",
        "    with open(local_dataset, \"r\") as dataset:\n",
        "        contents = unidecode.unidecode(dataset.read())\n",
        "        contents = contents.split('\\n')\n",
        "        contents = [content[:-1].strip() for content in contents]\n",
        "    return contents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlG1KgSMdmyt"
      },
      "source": [
        "### Declare all utilities\n",
        "This includes method to convert string to tensor + method for taking top 100,000 passwords from any dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "YoVsPI7sdmyt"
      },
      "source": [
        "# min size of ClixSense is 6\n",
        "# max size of ClixSense is 25\n",
        "# most common with their no_of_occurrences\n",
        "# [('123456', 17871), ('123456789', 3294), ('12345678', 2091), ('password', 1967), ('111111', 1892),\n",
        "# ('1234567', 1299), ('iloveyou', 1266), ('qwerty', 1187), ('clixsense', 1172), ('000000', 977)]\n",
        "\n",
        "def get_input_expected_clixsense(dataset: List[str]) -> Tuple[List[Tuple[str, int]], Dict[str, Set[Tuple[str, str]]]]:    \n",
        "    password_slices_dict = defaultdict(set)\n",
        "    # this was made to be in this way because I wanted to support more slices within a password.\n",
        "    # However, this is not being done. As a result this looks really stupid.\n",
        "    [password_slices_dict[pwd].add((pwd[0: -1], pwd[1:])) for pwd in dataset[:]]\n",
        "\n",
        "    n_most_common = 100000\n",
        "    all_passwords = [pwd for pwd in dataset]\n",
        "    counter = Counter(all_passwords)\n",
        "    most_common = counter.most_common(n_most_common)\n",
        "\n",
        "    return most_common, password_slices_dict\n",
        "\n",
        "\n",
        "def convert_str_to_tensor(string_to_convert: str):\n",
        "    size = len(string_to_convert)\n",
        "    converted_tensor = torch.zeros(size, 1).long()\n",
        "    for index, char in enumerate(string_to_convert):\n",
        "        converted_tensor[index][0] = string.printable.index(char)\n",
        "    return converted_tensor.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzImPFC7dmyu"
      },
      "source": [
        "### The GRU RNN network\n",
        "This model will be trained on 100,000 most common passwords in ClixSense for a set number of epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "FYWLe_4-dmyv"
      },
      "source": [
        "class GRU_RNN(nn.Module):\n",
        "    def __init__(self, embedding_dim: int, hidden_size: int, no_of_hidden_layers: int, output_size: int):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        # (L, N, H_in)\n",
        "        self.gru = nn.GRU(self.embedding_dim, hidden_size, num_layers=no_of_hidden_layers)\n",
        "        self.embedding = nn.Embedding(len(string.printable), embedding_dim=self.embedding_dim)\n",
        "        self.linear = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input: torch.Tensor, hidden_state: torch.Tensor):\n",
        "        input = self.embedding(input)\n",
        "        reshaped_input = input.view(1, 1, self.embedding_dim)\n",
        "        input, hidden_state = self.gru(reshaped_input, hidden_state)\n",
        "        output = self.linear(input)\n",
        "        return output, hidden_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUchXm3admyv"
      },
      "source": [
        "### Init the class which will train the RNN on ClixSense\n",
        "#### The rough logic for training is as follows:\n",
        "\n",
        "for 10 epochs\n",
        ">for password in 100,000 most occuring passwords in ClixSense\n",
        ">>form the input at t and the expected output at t + 1 for current password `[eg; for \"password\", input is \"passwor\" and expected is \"assword\"]` <br>\n",
        ">>for (num_of_occurrences for the current password / 100000) * 100 `[this is just to train passwords with more occurrences more times]`\n",
        ">>>train the GRU on the input, expected tensors of the current password"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Gx_sQy23dmyw"
      },
      "source": [
        "class PasswordGuesserUsingRNN:\n",
        "    def __init__(self):\n",
        "        self.hidden_size = 100\n",
        "        self.no_of_hidden_layers = 20\n",
        "        self.output_size = len(string.printable)\n",
        "        self.embedding_dim = 5\n",
        "        self.epochs = 10\n",
        "        self.eta = 1e-4\n",
        "\n",
        "    def train_and_evaluate(self):\n",
        "        dataset_klass = DatasetFactory().get(\"ClixSense\")\n",
        "        dataset = get_dataset(dataset_klass)\n",
        "\n",
        "        gru_model = GRU_RNN(self.embedding_dim, self.hidden_size, self.no_of_hidden_layers, self.output_size).to(device)\n",
        "        optimizer = torch.optim.Adam(gru_model.parameters(), lr=self.eta)\n",
        "        loss_fn = CrossEntropyLoss()\n",
        "\n",
        "        n_most_common, pwd_inp_exp = get_input_expected_clixsense(dataset)\n",
        "        # total_len of dict = 1338980\n",
        "        # total length of passwords = 2221027\n",
        "        for epoch in range(self.epochs):\n",
        "            for pwd_index, (most_common_pwd, num_occ) in enumerate(n_most_common[:]):\n",
        "                inp_target_set = pwd_inp_exp[most_common_pwd]\n",
        "                for _ in range(ceil((num_occ / 100000) * 100)):\n",
        "                    for input_pwd, target_pwd in inp_target_set:\n",
        "                        # this loop is always only run once as per current setting\n",
        "                        loss = 0\n",
        "                        optimizer.zero_grad()\n",
        "\n",
        "                        hidden_state = self._init_hidden()\n",
        "                        input_tensor = convert_str_to_tensor(input_pwd)\n",
        "                        target_tensor = convert_str_to_tensor(target_pwd)\n",
        "\n",
        "                        for input, expected in zip(input_tensor, target_tensor):\n",
        "                            output, hidden_state = gru_model(input, hidden_state)\n",
        "                            loss += loss_fn(output[-1], expected)\n",
        "\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                        loss = loss.item() / len(input_pwd)\n",
        "\n",
        "                if pwd_index % 1000 == 0:\n",
        "                    print(f\"At pwd_index: {pwd_index} of {len(n_most_common)}\")\n",
        "                    print(f\"training password: {most_common_pwd}\")\n",
        "\n",
        "                    print(f'At epoch: {epoch} with loss: {loss}')\n",
        "                    start = \"123\"\n",
        "                    prediction = self.evaluate_password(gru_model, start, 15)\n",
        "                    print(f\"Prediction is {prediction} for start with '{start}'\")\n",
        "\n",
        "        return gru_model\n",
        "\n",
        "    def evaluate_password(self, gru_model: nn.Module, password_start: str, max_length: int):\n",
        "        prediction = password_start\n",
        "        start_tensor = convert_str_to_tensor(password_start)\n",
        "        with torch.no_grad():\n",
        "            hidden_state = self._init_hidden()\n",
        "            for char in start_tensor:\n",
        "                _, hidden_state = gru_model(char, hidden_state)\n",
        "\n",
        "            input = start_tensor[-1]\n",
        "            for char_gen in range(max_length - len(password_start)):\n",
        "                output, hidden_state = gru_model(input, hidden_state)\n",
        "\n",
        "                # understand below; taken from the ref colab\n",
        "                output_dist = output.data.view(-1).exp()\n",
        "                top_i = torch.multinomial(output_dist, 1)[0]\n",
        "\n",
        "                char_predicted = string.printable[top_i]\n",
        "                prediction += char_predicted\n",
        "                input = convert_str_to_tensor(char_predicted)\n",
        "\n",
        "        return prediction\n",
        "\n",
        "    def _init_hidden(self):\n",
        "        # (D∗num_layers, N, Hout)\n",
        "        return torch.zeros(self.no_of_hidden_layers, 1, self.hidden_size).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aaCl3_idmyx"
      },
      "source": [
        "### Train the GRU model on ClixSense"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzrw-Knmdmyx"
      },
      "source": [
        "device = \"cuda\"\n",
        "gru_model = PasswordGuesserUsingRNN().train_and_evaluate()\n",
        "torch.save(gru_model, 'saved_gru_pwd.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgWMo4Hddmyy"
      },
      "source": [
        "The actual training was done locally and the logs for which can be found here: https://gist.github.com/venomouscyanide/8c4e18d042f4db891a614f838fa1b03a\n",
        "\n",
        "The training took approximately 11 hours to complete.\n",
        "After training, I saved the model for easily running experiments on the other 2 datasets.\n",
        "\n",
        "The saved model is hosted here: https://drive.google.com/file/d/1P5_RetiDuEh-dLMpPt_sdjrSc9fX9Pej/view?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no7fWaTpdmyz"
      },
      "source": [
        "## Init the class that will help make password guesses\n",
        "\n",
        "### The rough logic for creating passwords is as follows:\n",
        "\n",
        "\n",
        "for each password in list of most common passwords in ClixSense\n",
        ">for starting sequence taken by slicing the password with min length of 3`(eg: starting sequences of \"helloworld\" are ['hel', 'hell', 'hello', 'hellow', 'hellowo', 'hellowor', 'helloworl']`\n",
        ">>for sequence lengths from 5 to 15\n",
        ">>>for 5 guesses multiplied by \"importance\" factor `(guess more popular password sequences more)`\n",
        ">>>>guess the password given starting sequence and the sequence length\n",
        "\n",
        "importance factor = `ceil(no_of_occurrences / (dataset_len / importance) * 100)`\n",
        "<br>`importance` variable value was set to `4` in my experiments\n",
        "\n",
        "There is a counter kept for each guess and we break out of this segment when the counter reaches 1M guesses\n",
        "\n",
        "\n",
        "### After training the stats are printed and 3 debug text files are generated. \n",
        "The debug files are as follows:\n",
        "- starter passwords considered from ClixSense - `all_starters.txt`\n",
        "- passwords that were not guessed - `missed_passwords.txt`\n",
        "- all unique correctly guessed passwords - `unique_correct_guesses.txt`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "aOIOPmDjdmyz"
      },
      "source": [
        "class MakePasswordGuesses:\n",
        "    MIN_LENGTH: int = 5\n",
        "    MAX_LENGTH: int = 15\n",
        "    MAX_TRIES_PER_CONFIG: int = 5\n",
        "    MAX_GUESSES: int = int(math.pow(10, 6))\n",
        "\n",
        "    def __init__(self, model: nn.Module, verbose: bool = True):\n",
        "        self.model = model\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def evaluate_dataset(self, dataset_name: str) -> Tuple[Set[str], Set[str]]:\n",
        "        dataset_klass = DatasetFactory().get(dataset_name)\n",
        "        dataset = get_dataset(dataset_klass)\n",
        "\n",
        "        dataset_counter = Counter(dataset)\n",
        "        dataset_len = len(dataset)\n",
        "        print(f'Total passwords in {dataset_name} is {dataset_len}')\n",
        "        most_common = dataset_counter.most_common()\n",
        "\n",
        "        total_correct_guesses, guessed_passwords, all_starters_used = self._make_guesses(most_common, dataset_counter,\n",
        "                                                                                         dataset_len)\n",
        "        missed_passwords = set(dataset_counter.keys()).difference(guessed_passwords)\n",
        "\n",
        "        print(\n",
        "            f\"Unique guesses correct: {len(guessed_passwords)} and Total guesses: {total_correct_guesses} and total misses: {len(missed_passwords)}\"\n",
        "        )\n",
        "        print(f\"Coverage on {dataset_name}: {(total_correct_guesses / len(dataset)) * 100}\")\n",
        "\n",
        "        self._write_debug(dataset_name, \"all_starters.txt\", all_starters_used)\n",
        "        self._write_debug(dataset_name, \"unique_correct_guesses.txt\", guessed_passwords)\n",
        "        self._write_debug(dataset_name, \"missed_passwords.txt\", missed_passwords)\n",
        "\n",
        "        return guessed_passwords, missed_passwords\n",
        "\n",
        "    def _form_candidates(self, common_pwd: str) -> List[str]:\n",
        "        min_len = 3\n",
        "        return [common_pwd[0:end] for end in range(min_len, len(common_pwd))]\n",
        "\n",
        "    def _make_guesses(self, most_common: List[Tuple[str, int]], dataset_counter: Counter, dataset_len: int):\n",
        "        # TODO: Refactor. Super messy right now.\n",
        "        importance = 4\n",
        "        total_correct_guesses = 0\n",
        "        uniq_guessed_passwords = set()\n",
        "        all_starters_used = set()\n",
        "        total_guess_tracker = 0\n",
        "\n",
        "        seen_edge_cases = set()\n",
        "        guesser = PasswordGuesserUsingRNN()\n",
        "        for common_pwd, common_occ in most_common:\n",
        "            starter_candidates = self._form_candidates(common_pwd)\n",
        "            all_starters_used |= set(starter_candidates)\n",
        "\n",
        "            for candidate in starter_candidates:\n",
        "                if len(candidate) > self.MAX_LENGTH:\n",
        "                    if candidate not in seen_edge_cases:\n",
        "                        seen_edge_cases.add(candidate)\n",
        "                        if total_guess_tracker > self.MAX_GUESSES:\n",
        "                            return total_correct_guesses, uniq_guessed_passwords, all_starters_used\n",
        "\n",
        "                        total_guess_tracker += 1\n",
        "                        guess = candidate\n",
        "                        total_correct_guesses = self._update_if_guess_is_correct(uniq_guessed_passwords, guess,\n",
        "                                                                                 candidate, max_len,\n",
        "                                                                                 dataset_counter, total_correct_guesses)\n",
        "                    continue\n",
        "\n",
        "                for max_len in range(self.MIN_LENGTH, self.MAX_LENGTH + 1):\n",
        "\n",
        "                    if max_len < len(candidate):\n",
        "                        continue\n",
        "\n",
        "                    if max_len == len(candidate):\n",
        "                        if candidate not in seen_edge_cases:\n",
        "                            seen_edge_cases.add(candidate)\n",
        "                            if total_guess_tracker > self.MAX_GUESSES:\n",
        "                                return total_correct_guesses, uniq_guessed_passwords, all_starters_used\n",
        "\n",
        "                            total_guess_tracker += 1\n",
        "                            guess = candidate\n",
        "                            total_correct_guesses = self._update_if_guess_is_correct(uniq_guessed_passwords, guess,\n",
        "                                                                                     candidate, max_len,\n",
        "                                                                                     dataset_counter,\n",
        "                                                                                     total_correct_guesses)\n",
        "                        continue\n",
        "\n",
        "                    for _ in range(self.MAX_TRIES_PER_CONFIG * ceil(common_occ / (dataset_len / importance) * 100)):\n",
        "                        if total_guess_tracker > self.MAX_GUESSES:\n",
        "                            return total_correct_guesses, uniq_guessed_passwords, all_starters_used\n",
        "\n",
        "                        total_guess_tracker += 1\n",
        "                        if total_guess_tracker % 1000 == 0 and self.verbose:\n",
        "                            print(f\"At guess {total_guess_tracker} of {self.MAX_GUESSES}\")\n",
        "\n",
        "                        guess = guesser.evaluate_password(self.model, candidate, max_length=max_len)\n",
        "                        total_correct_guesses = self._update_if_guess_is_correct(uniq_guessed_passwords, guess,\n",
        "                                                                                 candidate, max_len,\n",
        "                                                                                 dataset_counter, total_correct_guesses)\n",
        "\n",
        "        return total_correct_guesses, uniq_guessed_passwords, all_starters_used\n",
        "\n",
        "    def _update_if_guess_is_correct(self, uniq_guessed_passwords: Set[str], guess: str, candidate: str, max_len: int,\n",
        "                                    dataset_counter: Counter, total_correct_guesses: int):\n",
        "        if guess not in uniq_guessed_passwords:\n",
        "            occurrences = dataset_counter.get(guess)\n",
        "            if occurrences:\n",
        "                if self.verbose:\n",
        "                    print(\n",
        "                        f\"Correct guess: {guess}, for candidate: {candidate}, given max_len: {max_len} \\nTotal Correct guesses so far:{total_correct_guesses}\"\n",
        "                    )\n",
        "                total_correct_guesses += occurrences\n",
        "                uniq_guessed_passwords.add(guess)\n",
        "        return total_correct_guesses\n",
        "\n",
        "    def _write_debug(self, dataset_name: str, file_name: str, data_as_set: Set[str]):\n",
        "        debug_folder = f'debug_{dataset_name}'\n",
        "        if not os.path.exists(debug_folder):\n",
        "            os.mkdir(debug_folder)\n",
        "\n",
        "        data_as_str = '\\n'.join(data_as_set)\n",
        "        with open(os.path.join(debug_folder, file_name), \"w\") as debug_file:\n",
        "            debug_file.write(data_as_str)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSvU3nrmdmyz"
      },
      "source": [
        "gru_model = torch.load('saved_gru_pwd.model')\n",
        "MakePasswordGuesses(gru_model).evaluate_dataset('Mate1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtRvWkategov"
      },
      "source": [
        "#### This was evaluated locally and the stats for this dataset are as follows:\n",
        "<br>Total time taken: ~7 hours(2GB MX150 GPU, 8 core i5-8250U CPU, 16 gigs of RAM)\n",
        "<br>Dataset: Mate1\n",
        "<br>Total no of passwords: 27398563\n",
        "<br>Total unique passwords: 9968589\n",
        "<br>Unique correct guesses: 41331\n",
        "<br>Total passwords guesses(includes duplicates): 4447756\n",
        "<br>Unique passwords misses(not guessed): 9927258 \n",
        "<br>Accuracy:  (4447756/27398563) * 100 =  **16.23353750340848%** \n",
        "<br>Link to guessing logs: https://gist.githubusercontent.com/venomouscyanide/a69beec369b3b15a2a63f1b98e3bfd90/raw/47901f126aeb61cba98e3424c662e6d40e4dc550/guessing_logs_mate1.log\n",
        "<br>Link to starter passwords text file: https://drive.google.com/file/d/1ddsIKMiagCdmDPqvbvGRAYdUX99BL-Xx/view?usp=sharing\n",
        "<br>Link to missed passwords text file: https://drive.google.com/file/d/17EfK3R0C4uFVGxLbbnKUPNhib-Kn5gc3/view?usp=sharing\n",
        "<br>Link to unique guesses passwords text file: https://drive.google.com/file/d/1CUPu8RR_rIaT9h0sU-jIwq_sfNheAm1q/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "PJBP823bdmy0"
      },
      "source": [
        "gru_model = torch.load('saved_gru_pwd.model')\n",
        "MakePasswordGuesses(gru_model).evaluate_dataset('000webhost')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMOz9tLJeg6H"
      },
      "source": [
        "#### The performance on 000webhost was rather poor. The stats are as follows:\n",
        "<br>Total time taken: ~7 hours(2GB MX150 GPU, 8 core i5-8250U CPU, 16 gigs of RAM)\n",
        "<br>Dataset: 000webhost\n",
        "<br>Total no of passwords: 14939552\n",
        "<br>Total unique passwords: 9805274\n",
        "<br>Unique correct guesses: 17591\n",
        "<br>Total passwords guesses(includes duplicates): 836296\n",
        "<br>Unique passwords misses(not guessed): 9787683\n",
        "<br>Accuracy:  (836296/14939552) * 100 = **5.597865317514207%**\n",
        "<br>Link to guessing logs: https://gist.githubusercontent.com/venomouscyanide/3c1b4837c2b462ba40fb4b08e27ebeb4/raw/a796f2ee3147fc1a3994c1ce7c5f79b29ac4bbbe/guessing_logs_000webhost.log\n",
        "<br>Link to starter passwords text file: https://drive.google.com/file/d/1QXFjM9whNOqPLxdwV7uJLDKK2XREvj8f/view?usp=sharing\n",
        "<br>Link to missed passwords text file: https://drive.google.com/file/d/1GsrBvZT9pYrIth2guSCV8yG6MroEJhbm/view?usp=sharing\n",
        "<br>Link to unique guesses passwords text file: https://drive.google.com/file/d/12VJV4Y0dWhUfrKNzJIf0U2i728sP-UJv/view?usp=sharing"
      ]
    }
  ]
}