# The forth project: 
This project involves a few activities to help you understand well how to model sequence data and predict sequence data. All activities are expected to be done in PyTorch.
- Do the exercises of this tutorial and make sure that you fully understand every bit of the code: https://colab.research.google.com/github/lcharlin/80-629/blob/master/week6-RNNs%2BCNNs/RNNs_Answers.ipynb#scrollTo=49GUzizBbllV
<br> Solve the problem in part 2 of the tutorial (i.e., Shakespeare's prediction)  with Transformer. Try to code Transformer with Pytorch yourself.<br> Compare the results of RNNs of Transformer. Which one does work better?
- Try to modify and train your RNN and Transformers for password guessing.
<br> Train your models on password dataset  Clixsense and try to guess the passwords of dataset Mate 1. You are allowed to make 1 million guess attempts.<br> Which percentage of passwords in Mate 1 are guessed by each of your models? <br>What is the highest guessing accuracy you can get on dataset Mate 1? 
<br> Use the same trained models and try to guess passwords in 000webhost dataset.  You are allowed to make 1 million guess attempts.<br> Which percentage of passwords in 000webhost are guessed by each of your models?<br> What is the highest guessing accuracy you can get on dataset 000webhost? 
## Readings:
- Chapter 10: https://www.deeplearningbook.org/
- Attention is All you need: https://arxiv.org/pdf/1706.03762.pdf (note: there are many websites/blogs trying to explain transformer and  attention, if you find some very useful share it with the group).
## Online Lectures:
- All the 4 weeks lectures of this course: https://www.coursera.org/learn/nlp-sequence-models
## Datasets:
- Shakespeare's dataset (the link is in colab tutorial)
- Clixsense: https://www.dropbox.com/s/sdccqebo9wcrahz/ClixSense.txt?dl=0
- Mate1: https://www.dropbox.com/s/xeqkfd663b6l4ju/Mate1.txt?dl=0
- 000webhost: https://www.dropbox.com/s/5lff9n1fqp2z3uo/000webhost.txt?dl=0
## Deadline:
 Share you code via Colab by Thursday 8th noon to everyone else, we meet on Friday 9th.
## Warning: 
This is one of the hardest projects, but very fun to do. So have fun and challenge yourself. This project has a potential to be extended to a publication! So do your best :D