# The forth project: 
This project involves a few activities to help you understand well how to model sequence data and predict sequence data. All activities are expected to be done in PyTorch.
1. Do the exercises of this tutorial and make sure that you fully understand every bit of the code: https://colab.research.google.com/github/lcharlin/80-629/blob/master/week6-RNNs%2BCNNs/RNNs_Answers.ipynb#scrollTo=49GUzizBbllV
2. Solve the problem in part 2 of the tutorial (i.e., Shakespeare's prediction)  with Transformer. Try to code Transformer with Pytorch yourself. Compare the results of RNNs of Transformer. Which one does work better?
3. Try to modify and train your RNN and Transformers for password guessing.
    1. Train your models on password dataset  Clixsense and try to guess the passwords of dataset Mate 1. You are allowed to make 1 million guess attempts. Which percentage of passwords in Mate 1 are guessed by each of your models? What is the highest guessing accuracy you can get on dataset Mate 1? 
    2. Use the same trained models and try to guess passwords in 000webhost dataset.  You are allowed to make 1 million guess attempts. Which percentage of passwords in 000webhost are guessed by each of your models? What is the highest guessing accuracy you can get on dataset 000webhost? 
## Readings:
- Chapter 10: https://www.deeplearningbook.org/
- Attention is All you need: https://arxiv.org/pdf/1706.03762.pdf (note: there are many websites/blogs trying to explain transformer and  attention, if you find some very useful share it with the group).
## Online Lectures:
- All the 4 weeks lectures of this course: https://www.coursera.org/learn/nlp-sequence-models
## Datasets:
- Shakespeare's dataset (the link is in colab tutorial)
- Clixsense: https://www.dropbox.com/s/sdccqebo9wcrahz/ClixSense.txt?dl=0
- Mate1: https://www.dropbox.com/s/xeqkfd663b6l4ju/Mate1.txt?dl=0
- 000webhost: https://www.dropbox.com/s/5lff9n1fqp2z3uo/000webhost.txt?dl=0
## Deadline:
 Share you code via Colab by Thursday 8th noon to everyone else, we meet on Friday 9th.
## Warning: 
This is one of the hardest projects, but very fun to do. So have fun and challenge yourself. This project has a potential to be extended to a publication! So do your best :D